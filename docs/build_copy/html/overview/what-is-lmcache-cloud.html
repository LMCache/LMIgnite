<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>What is LMCache Cloud? &#8212; lmcache-cloud-docs v0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=56c4699f" />
    <script src="../_static/documentation_options.js?v=2fea6348"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Architecture" href="architecture.html" />
    <link rel="prev" title="Overview" href="index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="what-is-lmcache-cloud">
<h1>What is LMCache Cloud?<a class="headerlink" href="#what-is-lmcache-cloud" title="Link to this heading">¶</a></h1>
<p>LMCache Cloud is a product solution that allows users to do 1-click deployment of <strong>vLLM production stack</strong> and <strong>LMCache</strong> as a cloud/on-prem hosted commercial cloud for fast and low latency genAI inference serving. It features a beautiful, intuitive web UI that can be hosted locally, providing easy management and monitoring of your LLM infrastructure across various cloud platforms.</p>
<section id="about-lmcache">
<h2>About LMCache<a class="headerlink" href="#about-lmcache" title="Link to this heading">¶</a></h2>
<p><strong>LMCache</strong> is an LLM serving engine extension designed to reduce Time-To-First-Token (TTFT) and increase throughput, especially under long-context scenarios. For comprehensive documentation, visit the <a class="reference external" href="https://docs.lmcache.ai/">LMCache documentation</a>. By storing the KV caches of reusable texts across various locations including GPU, CPU DRAM, and Local Disk, LMCache reuses the KV caches of any reused text (not necessarily prefix) in any serving engine instance, saving precious GPU cycles and reducing user response delay. According to the <a class="reference external" href="https://github.com/LMCache/LMCache">LMCache GitHub repository</a>, when combined with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.</p>
</section>
<section id="about-vllm-production-stack">
<h2>About vLLM Production Stack<a class="headerlink" href="#about-vllm-production-stack" title="Link to this heading">¶</a></h2>
<p>The <strong>vLLM production stack</strong> provides a comprehensive solution for enterprise-scale deployment of large language models in production environments. For detailed deployment instructions and configuration options, visit the <a class="reference external" href="https://docs.vllm.ai/en/stable/deployment/integrations/production-stack.html">vLLM Production Stack documentation</a>. It offers a complete ecosystem for serving LLMs with high performance, reliability, and scalability, including features like distributed inference, model management, and production-ready APIs. The <a class="reference external" href="https://github.com/vllm-project/production-stack">vLLM production stack</a> is designed to handle the complexities of deploying and managing LLM services at scale, making it ideal for enterprise applications requiring robust, high-throughput inference capabilities.</p>
</section>
<section id="key-performance-advantages">
<h2>Key Performance Advantages<a class="headerlink" href="#key-performance-advantages" title="Link to this heading">¶</a></h2>
<p>The key performance advantage brought by LMCache is the smart caching, migration, and pre-fetching of KV cache that does not need recomputation, especially for long-context scenarios. This intelligent caching mechanism significantly reduces computational overhead and latency by avoiding redundant calculations for previously processed content, making it particularly effective for applications with extensive context requirements.</p>
</section>
<section id="enterprise-features">
<h2>Enterprise Features<a class="headerlink" href="#enterprise-features" title="Link to this heading">¶</a></h2>
<p>LMCache Cloud provides comprehensive enterprise-grade features including:</p>
<ul class="simple">
<li><p><strong>Multi-tenancy Support</strong>: Secure isolation and resource management for multiple users or organizations</p></li>
<li><p><strong>LoRA Support</strong>: Efficient fine-tuning and adaptation capabilities for custom model variants</p></li>
<li><p><strong>MCP Support</strong>: Model Context Protocol integration for enhanced interoperability</p></li>
<li><p><strong>Autoscaling</strong>: Dynamic resource allocation based on demand patterns</p></li>
<li><p><strong>Smart Routing</strong>: Intelligent request distribution and load balancing across infrastructure</p></li>
<li><p><strong>High Availability</strong>: Built-in redundancy and failover mechanisms</p></li>
<li><p><strong>Security</strong>: Enterprise-grade security features and compliance capabilities</p></li>
</ul>
</section>
<section id="key-benefits">
<h2>Key Benefits<a class="headerlink" href="#key-benefits" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><strong>Performance</strong>: Significantly reduce latency for language model requests through intelligent KV cache management</p></li>
<li><p><strong>Scalability</strong>: Automatically scale to handle varying workloads with enterprise-grade infrastructure</p></li>
<li><p><strong>Reliability</strong>: High availability with built-in redundancy and failover mechanisms</p></li>
<li><p><strong>Cost Efficiency</strong>: Reduce API costs through intelligent caching and resource optimization</p></li>
</ul>
</section>
<section id="use-cases">
<h2>Use Cases<a class="headerlink" href="#use-cases" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Large language model applications</p></li>
<li><p>AI-powered chatbots and assistants</p></li>
<li><p>Content generation systems</p></li>
<li><p>Real-time language processing</p></li>
<li><p>Enterprise AI inference serving</p></li>
<li><p>Multi-tenant AI platforms</p></li>
</ul>
</section>
<section id="platform-and-vendor-support">
<h2>Platform and Vendor Support<a class="headerlink" href="#platform-and-vendor-support" title="Link to this heading">¶</a></h2>
<p>LMCache Cloud provides comprehensive support for major cloud platforms and orchestration systems:</p>
<ul class="simple">
<li><p><strong>Amazon Web Services (AWS)</strong></p>
<ul>
<li><p>Amazon Elastic Kubernetes Service (EKS)</p></li>
</ul>
</li>
<li><p><strong>Google Cloud Platform (GCP)</strong></p>
<ul>
<li><p>Google Kubernetes Engine (GKE)</p></li>
</ul>
</li>
<li><p><strong>Microsoft Azure</strong></p>
<ul>
<li><p>Azure Kubernetes Service (AKS)</p></li>
</ul>
</li>
<li><p><strong>On-Premises and Hybrid</strong></p>
<ul>
<li><p>Kubernetes clusters</p></li>
</ul>
</li>
<li><p><strong>Other GPU Vendors</strong></p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">lmcache-cloud-docs</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Overview</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">What is LMCache Cloud?</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html">Architecture</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/index.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Overview</a></li>
      <li>Next: <a href="architecture.html" title="next chapter">Architecture</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Yihua Cheng.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/overview/what-is-lmcache-cloud.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>